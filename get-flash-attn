#!/bin/bash

# This program reads info about your install and tries to find a matching flash attn wheel from
# https://github.com/Dao-AILab/flash-attention/releases

# An example wheel: flash_attn-2.7.0.post1+cu12torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl

# For this we need to know
#       Cuda version
#       torch version
#       cxx abi TRUE/FALSE
#       python version
#       OS (assume linux)
#       ISA

if command -v python3 &> /dev/null; then 
	PYTHON_CMD="python3" 
else 
	PYTHON_CMD="python" 
fi

printf "importing torch to parse config. could take a min...\n"
torch_config=`$PYTHON_CMD -c "import torch; print(torch.__config__.show())"` ||{
        printf "Error. could not import torch config. maybe torch can't be found? Have you activated your python evironment?\nexiting..."
        exit 1
}

# cut off everything up to and inc "CUDA_VERSION="      -> 12.4,CUDNN_VERSION=...
# takes up to the first '.'                             -> 12
cuda_version=`echo ${torch_config#*CUDA_VERSION=} | awk -F'.' '{print $1}'`
printf " +        cu$cuda_version\n"

# cut off everything up to and inc "TORCH_VERSION="             -> 2.5.1, USE_CUDA=ON, USE_CUDNN=ON,
# split along '.', take the 1st and 2nd match and add a '.'     -> 2.5
torch_version=`echo ${torch_config#*TORCH_VERSION=} | awk -F'.' '{print $1 "." $2 }'`
printf " +        torch v$torch_version\n"

if [ `echo $torch_version'>'2.7 | bc -l` -eq 1 ]; then
        printf "detected torch version '$torch_version' newer then latest known wheel '2.7'\nWill try get a v2.7 wheel\n"
        torch_version="2.7"
fi

# cut off everything up to and inc "ABI="
# then read the first char
ABI=`echo ${torch_config#*ABI=} | awk '{print substr ($0, 0, 1)}'`  # 0 or 1
if [[ ! $ABI -eq 0 ]] && [[ ! $ABI -eq 1 ]]; then
        printf "Error! ABI=$ABI, only '0' or '1' is valid. exiting...\n"
        exit 1
fi
#convert from 0/1 to TRUE/FALSE for whl name later
if [[ $ABI -eq 0 ]]; then
        ABI="FALSE"
else
        ABI="TRUE"
fi
printf " +        ABI$ABI\n"

# python version
# python3 --version                     -> Python 3.11.10
# cuts along ' ', takes the 2nd match   -> 3.11.10
# cuts along '.', takes the 2nd match   -> 11
python_version=`$PYTHON_CMD --version | awk -F' ' '{print $2}' | awk -F'.' '{print $2}'`
printf " +        cp${python_version}\n"

# OS
# Assume Linux. Always assume Linux
os="linux"
printf " +        $os\n"

# ISA
# Only x86_64 is available
isa="x86_64"
printf " +        $isa\n"
#assume 'x86' if lscpu cant be found
if  command -v lscpu 2>&1 >/dev/null; then
        isa=`lscpu | grep "^Architecture:" | awk '{print $2}'`
fi
if [[ $isa != "x86_64" ]]; then
        printf "Error! your ISA was detected to be '$isa'\nPrebuilt wheels for flash_attn are only available for 'x86_64'.\nExiting...\n"
        exit 0
fi


#check for internet
# if no, just run offline mode
wget -q --spider http://google.com
if [ ! $? -eq 0 ]; then
        printf "Warning! no internet detected (unable to ping 'http://google.com')\nRunning in offline mode instead\n"
        OFFLINE=1
fi

# Hardcoded - I can manually bump with new releases
flash_attn_version="2.7.4.post1"

# An example wheel: flash_attn-2.7.0.post1+cu12torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl
flash_attn_wheel="flash_attn-${flash_attn_version}+cu${cuda_version}torch${torch_version}cxx11abi${ABI}-cp3${python_version}-cp3${python_version}-${os}_${isa}.whl"
printf " =>       $flash_attn_wheel\n"

url="https://github.com/Dao-AILab/flash-attention/releases/download/v${flash_attn_version}/$flash_attn_wheel"
if [[ "$DRYRUN" -eq "1" ]] ; then
        printf "Dryrun! Printing commands instead:\n #        wget $url\n #        pip install $flash_attn_wheel\n"
elif [[ "$OFFLINE" -eq 1 ]]; then
        printf "Offline! To install flash-attn, please run 'wget ...' on a system with internet and copy it over and then run 'pip install':\n"
        printf "          wget $url\n"
        printf "          scp $flash_attn_wheel ...\n"
        printf "          pip install $flash_attn_wheel\n"
else
	printf "\nDownloading $flash_attn_wheel...\n"
        wget -q $url 2>&1 > /dev/null ||{
                printf "url not found! Maybe '$flash_attn_wheel' is not a supported configuration :(\nexiting...\n"
                exit 0
        }

	prefix=""
	if [[ ! $UV == "" ]]; then
		prefix="uv"
	fi

        $prefix pip install -q --no-cache-dir --no-deps $flash_attn_wheel
        rm $flash_attn_wheel

        pip list | grep flash_attn > /dev/null  2>get-flash-attn.err
	install_status=$?
	if [[ $install_status == "0" ]]; then
                printf "Install succeeded\n"
	else
                printf "Install failed. Your error is saved in 'get-flash-attn.err'\n"
		exit 0 
	fi
fi

if [ `echo $torch_version'>'2.6 | bc -l` -eq 1 ]; then
	printf "\nWarning. torch version '$torch_version' is 2.7 or higher. Due to changes in how torch is built, you might need to update your GCC for flash-attn before running\n"
	printf "On Atos: 		'export LD_LIBRARY_PATH=/usr/local/apps/gcc/12.2.0/lib64/:\$LD_LIBRARY_PATH'\n"
	printf "On Leonardo: 	'module load gcc/12.2.0'\n"
fi

printf "\nTesting flash attention install with \"python -c 'import flash_attn'\"\n"
python -c "import flash_attn" 2>get-flash-attn.err
import_status=$? 
if [[ $import_status == "0" ]]; then
	printf "Import succeeded\n"
else
	printf "Import failed. Your error is saved in 'get-flash-attn.err'\n"
	exit 0
fi
